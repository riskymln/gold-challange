{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__, template_folder='templateFiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e2caf",
   "metadata": {},
   "source": [
    "**API di Bawah untuk Cleansing Text (WORKS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/')\n",
    "def my_form():\n",
    "    return render_template('text_cleansing.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09546372",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['POST'])\n",
    "def my_form_post():\n",
    "    text = request.form['text']\n",
    "    clean_text = text.replace(\"USER\",\"\").replace(\"RT\",\"\").replace(\"URL\",\"\").lower().strip()\n",
    "    clean_text_2 = re.sub(r'[^a-zA-Z]',' ', clean_text)\n",
    "    total_char = len(clean_text_2)\n",
    "    total_word = len(clean_text_2.split())\n",
    "\n",
    "    abusive_words = []\n",
    "    with open('abusive.csv', 'r') as f:\n",
    "        for x in f:\n",
    "            if \";\" in x:\n",
    "                continue\n",
    "            elif len(x.strip())>0:\n",
    "                abusive_words.append(x.strip())\n",
    "\n",
    "    jumlah_abusive = 0\n",
    "    for kata in clean_text_2.split():\n",
    "        for abusive_word in abusive_words:\n",
    "            if kata == abusive_word:\n",
    "                jumlah_abusive += 1\n",
    "                break\n",
    "\n",
    "    is_abusive_sentence = []\n",
    "    if jumlah_abusive > 0:\n",
    "        is_abusive_sentence.append(\"Kalimat Abusive\")\n",
    "    else:\n",
    "        is_abusive_sentence.append(\"Bukan Kalimat Abusive\")\n",
    "\n",
    "    response = {\n",
    "    'data' : clean_text_2.split(),\n",
    "    'jumlah karakter' : total_char,\n",
    "    'jumlah kata' : total_word,\n",
    "    'jumlah abusive words' : jumlah_abusive,\n",
    "    'kesimpulan' : is_abusive_sentence\n",
    "    }\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d1030",
   "metadata": {},
   "source": [
    "**API di Bawah Untuk Cleansing File CSV (WORKS) --> pake data_latihan.csv untuk try**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/csv-upload', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/data', methods=['GET', 'POST'])\n",
    "def data():\n",
    "    if request.method == 'POST':\n",
    "        file = request.form.get('csvfile')\n",
    "        #data = csvfile\n",
    "        data = []\n",
    "        #with open(f) as file:\n",
    "        data = pd.read_csv(file)\n",
    "            #for row in csvfile:\n",
    "                #data.append(row)\n",
    "        #data = pd.DataFrame(data)\n",
    "        \n",
    "        data['total_char'] = data['Tweet'].apply(len)\n",
    "        data['total_word'] = data['Tweet'].apply(lambda x : len(x.split()))\n",
    "        data['1'] = data['Tweet'].apply (lambda x : x.replace(\"USER\", \"\").replace(\"RT\", \"\").replace(\"URL\", \"\").lower())\n",
    "        data['2'] = data['1'].astype(str).str.replace(r'[^a-zA-Z]', ' ', regex=True)\n",
    "        data['cleanse_tweet'] = data['2'].apply(lambda x:x.strip())\n",
    "        data['new_cleanse_tweet'] = data['cleanse_tweet'].apply (lambda x :x.split())\n",
    "        data['total_char_cleanse_tweet'] = data['cleanse_tweet'].apply(len)\n",
    "        data['total_word_cleanse_tweet'] = data['cleanse_tweet'].apply(lambda x : len(x.split()))\n",
    "        \n",
    "        abusive_words = []\n",
    "        with open('abusive.csv', 'r') as f:\n",
    "            for x in f:\n",
    "                if \";\" in x:\n",
    "                    continue\n",
    "                elif len(x.strip())>0:\n",
    "                    abusive_words.append(x.strip())\n",
    "        \n",
    "        unique_words = []\n",
    "        for sentence in data['new_cleanse_tweet']:\n",
    "            for word in sentence:\n",
    "                unique_words.append(word)\n",
    "                \n",
    "        unique_words = list(set(unique_words))\n",
    "                \n",
    "        word_to_index = {}\n",
    "        for index,word in enumerate(unique_words):\n",
    "            word_to_index[word] = index\n",
    "            \n",
    "        index_to_word = {}\n",
    "        for word,index in word_to_index.items():\n",
    "            index_to_word[index] = word\n",
    "        \n",
    "        for word in abusive_words:\n",
    "            try:\n",
    "                word_to_index[word]\n",
    "            except KeyError:\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                \n",
    "        index_to_word = {}\n",
    "        for word,index in word_to_index.items():\n",
    "            index_to_word[index] = word\n",
    "            \n",
    "        new_df_words = []\n",
    "        for sentence in data['new_cleanse_tweet']:\n",
    "            new_sentence = []\n",
    "            for word in sentence:\n",
    "                new_sentence.append(word_to_index.get(word, -1))\n",
    "            new_df_words.append(new_sentence)\n",
    "            \n",
    "        new_abusive_words = [word_to_index.get(word, -1) for word in abusive_words]\n",
    "        \n",
    "        jumlah_abusive_words_column = []\n",
    "        for row in (new_df_words):\n",
    "            jumlah_abusive_words = 0 \n",
    "            for abusive_word in new_abusive_words:\n",
    "                if abusive_word in row:\n",
    "                    jumlah_abusive_words = jumlah_abusive_words + 1\n",
    "            jumlah_abusive_words_column.append(jumlah_abusive_words)\n",
    "            \n",
    "        data['jumlah_abusive_words'] = jumlah_abusive_words_column\n",
    "        \n",
    "        is_abusive_sentence = []\n",
    "        for sentence in data['jumlah_abusive_words']:\n",
    "            if sentence > 0:\n",
    "                is_abusive_sentence.append(\"Kalimat Abusive\")\n",
    "            else:\n",
    "                is_abusive_sentence.append(\"Bukan Kalimat Abusive\")\n",
    "                \n",
    "        data['is_abusive_sentence'] = is_abusive_sentence\n",
    "        \n",
    "        data = data[['Tweet', 'total_char', 'total_word', 'cleanse_tweet', 'new_cleanse_tweet', 'total_char_cleanse_tweet', 'total_word_cleanse_tweet', 'jumlah_abusive_words', 'is_abusive_sentence']]\n",
    "        \n",
    "        return render_template('data.html', data=data.to_html(header=True, index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe5d4b",
   "metadata": {},
   "source": [
    "**API Untuk SQLite (masih BELUM WORKS (｡•́︿•̀｡))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c316081",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('cleansing_text.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute('''\n",
    "CREATE TABLE cleansing_text (text varchar(255) not null,\n",
    "                            text_cleansing varchar(255),\n",
    "                            total_char int,\n",
    "                            total_word int,\n",
    "                            jumlah_abusive_word int,\n",
    "                            conclusion varchar(255)\n",
    ");\n",
    "''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def cleansing_text():\n",
    "    msg = \"msg\"\n",
    "    if request.method == \"GET\":\n",
    "        try:\n",
    "            teks = text\n",
    "            data_cleansing = clean_text_2.split()\n",
    "            jumlah_char = total_char\n",
    "            jumlah_word = total_word\n",
    "            jumlah_abusive_word = jumlah_abusive\n",
    "            kesimpulan = is_abusive_sentence\n",
    "            with sqlite3.connect('cleansing_text.db') as con:\n",
    "                cur = con.cursor()\n",
    "                cur.execute('''INSERT into cleansing_text (text, text_cleansing, total_char, total_word, jumlah_abusive_word, conclusion) values (?, ?, ?, ?, ?, ?)''', (teks, data_cleansing, jumlah_char, jumlah_word, jumlah_abusive_word, kesimpulan))\n",
    "                con.commit()\n",
    "        except:\n",
    "            con.rollback()\n",
    "        finally:\n",
    "            con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ddd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
